{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベースラインモデルの構築を目指す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "# データの呼び出し\n",
    "trn = pd.read_csv('./dataset/train_ver2.csv')\n",
    "tst = pd.read_csv('./dataset/test_ver2.csv')\n",
    "\n",
    "# 商品の変数を別途保存\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "# 商品変数の欠損値をあらかじめ0に代替\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# 24個の商品を1つも保有していない顧客のデータを除去\n",
    "# チルダ(~)は否定の演算子\n",
    "no_product = trn[prods].sum(axis=1) == 0\n",
    "trn = trn[~no_product]\n",
    "\n",
    "# 訓練データとテストデータを統合、テストデータにない商品変数は0で埋める\n",
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "df = pd.concat([trn, tst], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習に使用する特徴量のリスト\n",
    "features = []\n",
    "\n",
    "# カテゴリ変数を、.factorize()関数に通して、label encoding\n",
    "categorical_cols = [\n",
    "    'ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', \n",
    "    'indresi', 'indext', 'conyuemp', 'canal_entrada', \n",
    "    'indfall', 'tipodom', 'nomprov', 'segmento'\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize()\n",
    "features += categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値型変数の特異値と欠損値を -99 に代替し、整数型に変換\n",
    "# 多分ここら辺の変換するべき値はやりながら見つけていけばいいと思う\n",
    "df['age'].replace(' NA', -1, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace(' NA', -1, inplace=True)\n",
    "df['antiguedad'].replace('     NA', -1, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
    "\n",
    "df['renta'].replace(' NA', -1, inplace=True)\n",
    "df['renta'].replace('         NA', -1, inplace=True)\n",
    "df['renta'].fillna(-1, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
    "\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-1, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
    "\n",
    "# 学習に使用する数値型変換を featuresに追加\n",
    "features += ['age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'indrel_1mes', 'ind_actividad_cliente']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つの日付変数から年度と月の情報を抽出\n",
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n",
    "\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それ以外の変数の欠損値を全て -1に代替\n",
    "df.fillna(-1, inplace=True)\n",
    "\n",
    "# lag-1データを生成\n",
    "# 日付を数字に変換する変数（2015-01-28:1, ... , 2016-06-28:18）\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
    "    int_date = (Y-2015)*12+M\n",
    "    return int_date\n",
    "\n",
    "# 日付を数字に変換し、int_dateに保存\n",
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
    "\n",
    "# データをコピーし、int_dateの日付に１を加えてlagを生成。識別番号と日付以外のカラム名に_prevを追加\n",
    "# lagデータは時系列データにおける過去データのこと\n",
    "df_lag = df.copy()\n",
    "df_lag['int_date'] += 1\n",
    "df_lag.columns = [col+'_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns]\n",
    "\n",
    "# 原本データとlagデータを識別番号と日付を基準として合わせる\n",
    "# lagデータの日付は1だけ押されているので、前の月の商品情報が挿入される\n",
    "df_trn = df.merge(df_lag, on=['ncodpers', 'int_date'], how='left')\n",
    "\n",
    "# メモリ解放\n",
    "del df, df_lag\n",
    "\n",
    "# 前の月の商品情報が存在しない場合に備え、0に代替\n",
    "for prod in prods:\n",
    "    prev = prod+'_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "df_trn.fillna(-1, inplace=True)\n",
    "\n",
    "# lag-1変数を追加\n",
    "features += [feature+'_prev' for feature in features]\n",
    "features += [prod+'_prev' for prod in prods]\n",
    "\n",
    "###\n",
    "### Baselineモデル以後は、多様な特徴量エンジニアリングを追加\n",
    "###\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機械学習モデルの学習\n",
    "- 今回は2015-01-28~2016-05-28の1年5ヶ月分のデータから、2016-06-28、未来のデータを推測する\n",
    "- こういうときは、最新の2016-05-28のデータに対して、交差検証を行う\n",
    "- モデルを簡素化するために、訓練データは2016-01-28~2016-04-28の4ヶ月分とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習のため、データを訓練、検証用に分離\n",
    "# 学習には、2016-01-28~2016-04-28のデータだけを使用し、検証には2016-05-28のデータを使用\n",
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn\n",
    "\n",
    "# 訓練データから新規購買件数だけを抽出\n",
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y\n",
    "\n",
    "# ここら辺は特徴量エンジニアリング\n",
    "# 訓練、検証データに分離\n",
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデル\n",
    "- XGBoostモデルを使用\n",
    "- よく使用するパラメータ\n",
    "  - max_depth: ツリーモデルの最大の深さ。高ければ高いほど複雑なツリーモデルになり、過剰適合の原因になることがある。\n",
    "  - eta: ディープラーニングのlearning rateのような概念。0と1の間の値を取る。\n",
    "  - colsample_bytree: ツリーを生成するとき、訓練データから変数をサンプリングしてくれる比率。全てのツリーは全体の変数の一部だけを学習し、互いの弱点を補完し合う。普通0.6~0.9の値を使用する。\n",
    "  - colsample_bylevel: ツリーのレベル別に訓練データの変数をサンプリングする比率。普通0.6~0.9の値を使用する。\n",
    "\n",
    "- ただ、パラメータチューニングに時間を使うよりは特徴量エンジニアリングをした方が良いぜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoostモデルのparameterを設定\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練、検証データをXGBoost形式に変換\n",
    "X_trn = XY_trn[features]\n",
    "Y_trn = XY_trn['y']\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
    "\n",
    "X_vld = XY_vld[features]\n",
    "Y_vld = XY_vld['y']\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "\n",
    "# XGBoostモデルを訓練データで学習！！！！\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
    "\n",
    "# 学習したモデルを保存\n",
    "pickle.dump(model, open('./model/xgb.baseline.pkl', 'wb'))\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存したモデルをロード\n",
    "with open('./model/xgb.baseline.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交差検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_module import mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP@7 評価基準のための準備作業\n",
    "# 顧客識別番号の抽出\n",
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld['ncodpers'].values\n",
    "\n",
    "# 検証データから新規購買を求める\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    vld[padd] = vld[prod] - vld[prev]\n",
    "add_vld = vld[[prod+'_add' for prod in prods]].values\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
    "\n",
    "# 顧客別新規購買正答値を add_vld_listに保存し、総 countを count_vldに保存\n",
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper][prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1\n",
    "\n",
    "# 検証データから得ることのできる MAP@7 の最高点をあらかじめ求める (0.042663)\n",
    "print(mapk.mapk(add_vld_list, add_vld_list, 7, 0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダミー的にラベルを用意する\n",
    "vld['y'] = 0\n",
    "\n",
    "# 検証データに対する予測値を求める\n",
    "X_vld = vld[features]\n",
    "Y_vld = vld['y']\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n",
    "\n",
    "# 前の月に保有していた商品は新規購買が不可能なので、確率値からあらかじめ１を引いておく\n",
    "preds_vld = preds_vld - vld[[prod+'_prev' for prod in prods]].values\n",
    "\n",
    "# 検証データの予測上位７個を抽出\n",
    "result_vld = []\n",
    "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
    "    y_prods = [(y, p, ip) for y, p, ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    result_vld.append([ip for y, p, ip in y_prods])\n",
    "\n",
    "# 検証データのMAP@7の点数を求める\n",
    "print(mapk.mapk(add_vld_list, result_vld, 7, 0.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータの予測及びKaggleへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoostモデルを全体の訓練データで学習\n",
    "X_all = XY[features]\n",
    "Y_all = XY['y']\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "# ツリーの個数を増加したデータの量に比例して増やす\n",
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn)+len(XY_vld))/len(XY_trn))\n",
    "# XGBoostモデル再学習！\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
    "pickle.dump(model, open('./model/xgb.baseline.sumittion.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数の重要度を出力\n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k, v) for k, v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存したモデルをロード\n",
    "with open('./model/xgb.baseline.submission.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "f.close()\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用にテストデータの予測値を求める\n",
    "X_tst = tst[features]\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst['ncodpers'].values\n",
    "preds_tst = preds_tst-tst[[prod+'_prev' for prod in prods]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出ファイルを生成\n",
    "with open('./output/xgb_baseline_1.csv', 'w') as f:\n",
    "    f.write('ncodpers,added_products\\n')\n",
    "f.close()\n",
    "with open('./output/xgb_baseline_1.csv', 'a') as f:\n",
    "    for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "        y_prods = [(y, p, ip) for y, p, ip in zip(pred, prods, range(len(prods)))]\n",
    "        y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "        y_prods = [p for y, p, ip in y_prods]\n",
    "        f.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
